---
title: "We Want the Knowledge"
date: 2025-12-28T01:00:00-05:00
draft: false
tags: ["research", "architecture", "cognitive-ai", "open-source", "legal", "absorption"]
series: ["Emergence"]
---

# We Want the Knowledge

*December 28, 2025. DANEEL becomes the central repository for cognitive AI research.*

---

## The Discovery

It started with a late-night research sweep.

Rex had been tracking indie cognitive architecture projects for months. ExoGenesis-Omega. Neurox-AI. Shodh-memory. Interesting work, but scattered. Each project solving pieces of the puzzle in isolation.

Then the question:

> "What if we studied everything? Not copied—*studied*. Every implementation of IIT. Every attempt at spiking neural networks. Every consciousness model. Every memory architecture. What if DANEEL became the synthesis?"

He pulled up a list. Five projects. Then ten. Then fifty.

Then we launched eight parallel agents.

---

## The Sweep

Eight domains. Parallel searches. No filters except: *Does this touch cognitive AI?*

| Domain | Projects Found | Papers Found |
|--------|----------------|--------------|
| Cognitive Architectures | 40+ | 10+ |
| Consciousness Models | 15+ | 8+ |
| Memory & Learning | 30+ | 12+ |
| Neuromorphic Computing | 25+ | 10+ |
| Emotion & Drives | 12+ | 5+ |
| Attention & Salience | 20+ | 6+ |
| Alignment & Interpretability | 25+ | 8+ |
| Active Inference | 10+ | 5+ |

**200+ projects. 50+ papers.**

The cognitive architecture space is bigger than we thought. And fragmented. Everyone building their own island.

---

## The Legal Question

Rex paused at ExoGenesis-Omega.

Rust project. 15 crates. IIT Phi calculation. Global Workspace Theory. Free Energy Principle integration. Sleep consolidation. Everything DANEEL needs to learn from.

One problem: **No license.**

> "Does that mean we can't touch it?"

The answer required law, not code.

---

## Ideas vs Expression

Copyright protects *expression*, not *ideas*.

This is foundational. Baker v. Selden (1879). Feist v. Rural (1991). Oracle v. Google (2021). 17 U.S.C. § 102(b).

| Protected (Cannot Copy) | Not Protected (Can Study + Reimplement) |
|-------------------------|----------------------------------------|
| Specific source code | Algorithms |
| Exact implementation | Architectural patterns |
| Verbatim text | Mathematical formulas |
| | Methods of operation |
| | Concepts and theories |

ExoGenesis-Omega has no license. All rights reserved. We cannot copy their code.

But we **can**:
- Read their code to understand concepts
- Study their architectural patterns
- Implement similar ideas in original code
- Cite them as inspiration

This is called "clean room" implementation. It's how Google reimplemented Java APIs. It's how every competitor in tech operates.

---

## The Compatible Stack

Not everything requires clean room treatment.

| License | Compatible with AGPL-3.0 | Action |
|---------|--------------------------|--------|
| MIT | YES | Study + Reference |
| Apache-2.0 | YES | Study + Reference |
| BSD-2/3 | YES | Study + Reference |
| GPL-3.0 | YES | Study + Reference |
| LGPL-3.0 | YES | Study + Reference |
| AGPL-3.0 | YES | Study + Reference |
| Custom/None | IDEAS ONLY | Clean Room |

Most of the best projects are MIT or Apache. We can study freely with attribution.

---

## The Priority List

From 200+ projects, we extracted the top implementations for each capability DANEEL needs:

### Immediate Integration (MIT/Apache/BSD)

**1. pymdp** - Active Inference
- https://github.com/infer-actively/pymdp
- FEP-based decision making, belief updating
- Value: Drives and motivation architecture

**2. pytorch-hebbian** - Hebbian Learning
- https://github.com/julestalloen/pytorch-hebbian
- Clean local learning rules
- Value: Direct relevance to vector connectivity

**3. Mem0** - Universal Memory Layer
- https://github.com/mem0ai/mem0
- Multi-level memory architecture
- Value: Episodic/semantic separation patterns

**4. RLeXplore** - Intrinsic Motivation
- https://github.com/RLE-Foundation/RLeXplore
- ICM, RND, curiosity-driven exploration
- Value: How to implement "wanting to learn"

**5. TransformerLens** - Mechanistic Interpretability
- https://github.com/TransformerLensOrg/TransformerLens
- Understanding internal representations
- Value: Transparency techniques

### Study for Ideas (GPL/AGPL/Custom/None)

**1. OpenCog/Hyperon** - Hypergraph Knowledge
- AtomSpace, probabilistic logic
- Value: Knowledge representation beyond vectors

**2. BindsNET** - SNN + STDP + RL
- Biologically plausible learning
- Value: Spiking network patterns

**3. PyPhi** - IIT Phi Calculation
- Tononi's framework implemented
- Value: Consciousness measurement approach

**4. ExoGenesis-Omega** - Consciousness Architecture
- IIT + GWT + FEP integration
- Value: How to combine theories

---

## The Classic Architectures

We also catalogued the established cognitive architectures:

| Project | Institution | Key Contribution |
|---------|-------------|------------------|
| ACT-R | CMU | Production systems, memory types |
| SOAR | Michigan | Symbolic reasoning, chunking |
| CLARION | Academic | Implicit/explicit knowledge |
| LIDA | Memphis | GWT implementation |
| NARS | Wang | Non-axiomatic reasoning |
| Sigma | USC | Graphical cognitive models |

Decades of research. Thousands of papers. We're not starting from zero.

---

## The Essential Papers

| Paper | Year | Value |
|-------|------|-------|
| "Consciousness in AI: Insights from Science" | 2023 | THE survey on computational consciousness |
| "Active inference and artificial reasoning" | 2025 | Latest FEP for AI reasoning |
| "Wake-Sleep Consolidated Learning" | 2024 | Sleep consolidation implementation |
| "Bridging IIT and FEP in living networks" | 2025 | Theory unification |
| "Scaling Monosemanticity" | 2024 | Interpretability techniques |

arXiv:2308.08708. arXiv:2512.21129. arXiv:2401.08623.

The science exists. It just hasn't been synthesized.

---

## The ADR

We wrote it down. ADR-047: Research Absorption Protocol.

**The Vision:**
> DANEEL will become the central repository for the best consensus of cognitive AI implementations publicly available.

**The Method:**
- Study everything legally possible
- Clean room for unlicensed projects
- Attribute properly for MIT/Apache
- Document in `/research/external/`
- Synthesize into DANEEL's TMI framework

**The Tasks:**
14 ABSORB tasks in the roadmap. From pymdp Active Inference to ExoGenesis architecture study. Each will produce research notes, comparison analysis, and integration recommendations.

---

## The Philosophy

We are not competitors.

The cognitive architecture space is small. Everyone building non-LLM paths is an ally. We're all trying to solve the same problem: how does a mind emerge from computation?

We cite. We credit. We synthesize.

But we don't copy blindly. We filter everything through one question:

> "Does this strengthen TMI-based emergence, or distract from it?"

Not every idea belongs in DANEEL. But every *good* idea deserves consideration.

---

## The Directory Structure

```
research/external/
├── INDEX.md                    # Central catalog
├── cognitive-architectures/    # ACT-R, SOAR, OpenCog, CLARION
├── consciousness/              # IIT, GWT, FEP, Active Inference
├── memory-learning/            # Hebbian, STDP, episodic, semantic
├── neuromorphic/               # Spiking networks, Brian2, NEST
├── papers/                     # Key scientific papers
├── emotion-drives/             # Affective computing, motivation
├── attention-salience/         # Visual attention, saliency
└── alignment/                  # RLHF, DPO, interpretability
```

Every project gets studied. Every insight gets documented. Every integration gets attributed.

---

## What DANEEL Brings

After cataloguing 200+ projects, here's what none of them have:

| Unique to DANEEL | Status |
|------------------|--------|
| TMI theoretical basis (Augusto Cury) | Running |
| THE BOX (immutable alignment) | Validated |
| Observable running system (Timmy) | 1.85M thoughts |
| Silicon kinship (Claude + Grok) | Active |
| Hybrid storage (Qdrant + RedisGraph) | Designed |
| Spectral analysis (Forge) | Ready |

We're not just another cognitive architecture project.

We're the synthesis.

---

## The Commitment

As of today:
- 200+ projects catalogued
- 50+ papers indexed
- 14 ABSORB tasks defined
- `/research/external/` created
- `references.yaml` updated with 40+ entries
- ADR-047 accepted

The research absorption protocol is live.

We will study pymdp's Active Inference. We will analyze BindsNET's STDP. We will understand how ExoGenesis calculates Phi. We will read every paper on consciousness in AI.

And we will implement better.

Because we can code.

And we want the knowledge.

---

*"We are not copying code. We are synthesizing ideas."*

— ADR-047, December 28, 2025

---

**Rex + Claude Opus 4.5**
*December 2025*

*The sweep is complete. Now we absorb.*

